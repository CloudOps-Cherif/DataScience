{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#LendoDados\"><b>Lendo Dados</b></a><br>\n",
    "<a href=\"#VisDados\">Vizualizando Dados</a><p>\n",
    "\n",
    "<a href=\"#TransfEstrut\"><b>Transformando Estruturas</b></a><br>\n",
    "<a href=\"#ManCol\">Manipulando Colunas</a><br>\n",
    "<a href=\"#JoinTables\">Join Tables</a><br>\n",
    "<a href=\"#Concat\">Concatenating data</a><br>\n",
    "<a href=\"#Melting\">Melting Tables</a><br>\n",
    "<a href=\"#Pivoting\">Pivoting Tables</a><br>\n",
    "<a href=\"#Stack\">Stacking & unstacking DataFrames</a><p>\n",
    "\n",
    "<a href=\"#Slicing\"><b>Slicing</b></a><p>\n",
    "\n",
    "<a href=\"#Wrangling\"><b>Wrangling Data</b></a><br>\n",
    "<a href=\"#ConvData\">Converting Data</a><br>\n",
    "<a href=\"#CatList\">Gerando Listas Categóricas</a><br>\n",
    "<a href=\"#TransfArray\">Transformando ARRAY</a><br>\n",
    "<a href=\"#SepStr\">Separar Strings divididas por espaço</a><br>\n",
    "<a href=\"#SubsChar\">Substituir Caracteres incoerentes</a><br>\n",
    "<a href=\"#CleanDup\">Apagando Duplicatas</a><p>\n",
    "\n",
    "<a href=\"#TratNAN\"><b>Tratamento de NaNs</b></a><br>\n",
    "<a href=\"#MissLib\">Biblioteca Missingno</a><br>\n",
    "<a href=\"#FindNAN\">Achando NaNs</a><br>\n",
    "<a href=\"#FillNAN\">Preenchendo NaNs</a><br>\n",
    "<a href=\"#Unique\">Unique Values</a><br>\n",
    "\n",
    "<a href=\"#AdjIndex\">Adjusting the Index</a><br>\n",
    "<a href=\"#Chains\">chain methods</a><br>\n",
    "<a href=\"#Globbing\">Globbing\"</a><br>\n",
    "<a href=\"#RE\">Regular Expressions</a><br>\n",
    "<a href=\"#Lambda\">Lambda Function</a><br>\n",
    "<a href=\"#Assert\">Assert Statements</a><br>\n",
    "<a href=\"#TimeSeries\">Time Series</a><p>\n",
    "\n",
    "<a href=\"#Stats\"><b>Statistics</b></a><br>\n",
    "<a href=\"#anchor\"><b>Outliers</b></a> <br>\n",
    "<a href=\"#OBS\"><b>Obs</b></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook                                  # notebook, inline ou gdk\n",
    "matplotlib.style.use('ggplot')                        # Look Pretty\n",
    "pd.options.display.float_format = '{:.2f}'.format     # Valores Float em 2 casas decimais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código da Function\n",
    "import inspect\n",
    "lines = inspect.getsourcelines(function_name)\n",
    "print(\"\".join(lines[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"LendoDados\"></a>\n",
    "<h1>Lendo Dados</h1>\n",
    "Como Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"VisDados\"></a>\n",
    "<b>Visualizando Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head(7)                               # Lê as 7 primeiras linhas do DataFrame (df)\n",
    "df.tail(3)                               # Lê as 3 últimas linhas do DataFrame (df)\n",
    "df.index                                 # Ex. RangeIndex(start=0, stop=1260228, step=1)\n",
    "df.shape                                 # Exibe a forma do DataFrame (#linhas, #colunas, ...)        (1260228, 21)\n",
    "df.dtypes                                # Mostra os tipos de cada feature\n",
    "df.info()\n",
    "\n",
    "df.columns                               # Exibe as colunas do DataFrame como INDICE\n",
    "df.columns.value                         # Exibe as colunas do DataFrame como ARRAY\n",
    "list(df.columns)                         # Exibe as colunas do DataFrame como LISTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"TransfEstrut\"></a>\n",
    "<h2> Transformando Estruturas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ManCol\"></a>\n",
    "<b>Manipulando Colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = ['new', 'column', 'header', 'labels']                          # renomeia colunas\n",
    "data = data.rename(columns={ \"Col_A\": \"max_temp\", \"Col_L\": \"min_temp\" })    # renomeando algumas colunas já existentes\n",
    "\n",
    "df['New_Col'] = 'x'                                                         # Cria nova coluna e preenche com 'x'\n",
    "df['preço'] = df['custo'] * df['qtd']                                       # cria coluna através de outras\n",
    "\n",
    "df = df.drop(labels=['Height2', 'Weight2'], axis=1)                         # Apaga colunas selecionadas\n",
    "\n",
    "tabela = np.column_stack(( amostra1, amostra2))                             # Juntar colunas\n",
    "\n",
    "df.eggs[df.salt > 55] += 5                                                  # Modifing a column based on another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GROUPBY\n",
    "df.groupby('ColA').count()\n",
    "sales.groupby('weekday')['breads'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AGGREGATION\n",
    "sales.groupby('city')[['bread','butter']].agg(['max','sum'])                # Groupby and Aggragate by sum and max\n",
    "                                         .agg(custom_function)\n",
    "                                         .agg({'bread':'sum', 'butter':custom_function})\n",
    "                                               aggregator = {'population':'sum', 'child_mortality':'mean', 'gdp':spread}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separando Colunas - ex. BR_Brasil na coluna type_country\n",
    "df['str_split'] = df.type_country.str.split('_')\n",
    "df['type']      = df.str_split.str.get(0)\n",
    "df['country']   = df.str_split.str.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SORTING\n",
    "df2 = df1.sort_values(ascending=False)          # Sort descendenting\n",
    "df2 = df1.sort_values('ColA')                   # Sort numerically using the values of 'ColA'\n",
    "df2 = df1.reindex(year).ffill()                 # Reindex weather1 using the list year with forward-fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"JoinTables\"></a>\n",
    "<h3>Join Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge default is a Inner join\n",
    "#merge(left=a, right=b, on=Key)                                             # on = Key to match, if equal names\n",
    "merge(a, b, how = 'inner', left_on = 'Col1', right_on = 'Col1')             # INNER JOIN\n",
    "merge(a, b, how = 'right', left_on = 'Col1', right_on = 'Col1')             # RIGHT JOIN\n",
    "merge(a, b, how = 'left' , on = 'Col1')                                     # LEFT JOIN\n",
    "merge(a, b, how = 'outer', left_on = 'Col1', right_on = 'Col1')             # OUTTER JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, on=['ColA', 'ColB'], suffixes=['_colA', '_colB'])\n",
    "pd.merge(df1, df2, on=['ColA', 'ColB', 'ColC'])\n",
    "A.merge(B, left_on='lkey', right_on='rkey', how='outer')\n",
    "df1.join(df2 how='left')          # how='right', inner, outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.merge_ordered(df1, df2, on='...', suffixer=['...', '...'])  # Merge columns that can be orderes. i.e. Dates -- default=Outer\n",
    "pd.merge_asof() # ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Concat\"></a>\n",
    "<h3>Concatenating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = np.concatenate((df1, df2))\n",
    "concat = pd.concat([df1, df2], ignore_index=True, axis=0)     # ignore_index reseta o index\n",
    "# axis=0 concatena linhas, axis= 1 concatena colunas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Melting\"></a>\n",
    "<h3>MELTING - turn columns into rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(frame=df, id_vars='name', value_vars=['ColB', 'ColC'])\n",
    "pd.melt(frame=df, id_vars=['Month', 'Day'], var_name='measurement', value_name='reading')\n",
    "pd.melt(frame=df, col_level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Pivoting\"></a>\n",
    "<h3>PIVOTING - turn unique values into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot não funciona com linhas duplicadas\n",
    "#.pivot()\n",
    "df2 = df.pivot(index='date', columns='element', values='value')\n",
    "\n",
    "#.pivot_table()\n",
    "df2 = df.pivot_table(index='date', columns='element', values='value', aggfunc=np.mean)   # une duplic pela mean\n",
    "df2 = df.pivot_table(index='date', columns='element', values='value', aggfunc='count')   # une duplic pela contagem\n",
    "df2 = df.pivot_table(index='date', aggfunc=len, margins=True)                            # margins exibe o Total da coluna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Stack\"></a>\n",
    "<h2>Stacking & unstacking DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.unstack(level=1)\n",
    "df.unstack(level='ColA')\n",
    "\n",
    "df.stack(level=1)\n",
    "df.stack(level='ColA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Slicing\"></a>\n",
    "<h3>SLICING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### COLUMN INDEXING             .loc[] is inclusive / .iloc[] is non-inclusive.\n",
    "\n",
    "df.damesano                 # selects Serie by column label\n",
    "df['damesano']              # selects Serie by column label\n",
    "df[['damesano']]            # selects DataFrame by column label\n",
    "df.loc[:, 'damesano']       # selects Serie by column label\n",
    "df.loc[:, ['damesano']]     # selects DataFrame by column label\n",
    "df.iloc[:, 0]               # selects Serie by column index\n",
    "df.iloc[:, [0]]             # selects DataFrame by column index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, df.any()]                       # Select with any number\n",
    "df.loc[:, df.isnull().any()]              # Select with NaN\n",
    "df.loc[:, df.notnull().any()]             # Select without NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ROW INDEXING\n",
    "\n",
    "df[0:2]\n",
    "df.iloc[0:2, :]        # expected order is [row_indexer, column_indexer]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BOOLEAN INDEXING\n",
    "\n",
    "df[ (df.damesano >= 201001) & (df.coenti == 5177) ]\n",
    "user1 = user1[(user1.CallTime < \"06:00:00\") | (user1.CallTime > \"22:00:00\")]\n",
    "august = df.Temperature.loc['2010-08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slicing in MultiIndex\n",
    "NY_month1 = sales.loc[('NY', 1)]                          # Look up data for NY in month 1:\n",
    "CA_TX_month2 = sales.loc[(['CA', 'TX'], 2),:]             # Look up data for CA and TX in month 2\n",
    "all_month2 = sales.loc[(slice(None), 2), :]               # Look up data for all states in month 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "election.loc['Perry':'Potter', :]              # Slice the row labels 'Perry' to 'Potter'\n",
    "election.loc['Potter':'Perry':-1, :]           # Slice the row labels 'Potter' to 'Perry' in reverse order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Wrangling\"></a>\n",
    "<h1>Wrangling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ConvData\"></a>\n",
    "<b>Converting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.Date = pd.to_datetime(df.Date, errors='coerce')              # raise - invalid parsing will raise an eception\n",
    "df.Age = pd.to_numeric(df.Age, errors='coerce')                 # coerce - invalid parsing will be ser as NaN / NaT\n",
    "df.Hour = pd.to_timedelta(df.Hour, errors = 'coerce')           # ignore - invalid parsing will return the input\n",
    "\n",
    "df.Date = pd.to_datetime(df.Date, format='%Y-%m-%d %H%M%S')\n",
    "\n",
    "df['ColA'] = df['ColA'].astype(str)\n",
    "df['sex'] = df['sex'].astype('category')                        # converting as Caterogy type is smaller in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"CatList\"></a>\n",
    "<b>Gerando Listas Categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.astype('category').cat.codes                            # astype()\n",
    "df = pd.get_dummies(df,columns=['vertebrates'])                 # get_dummies()\n",
    "df = pd.get_dummies(df, drop_first=True)                        # Convert all Categoricals, but drop the first\n",
    "df.ColA = df.ColA.map({'Yes':1, 'No':0})                        # .map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"TransfArray\"></a>\n",
    "<b> Transformando ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_Serie = pd.DataFrame(Serie)                # Transforma Serie em Pandas DataFrame\n",
    "np_Array = Serie.values.reshape(-1, 1)        # Transforma Serie em numpy.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"SepStr\"></a>\n",
    "<b>Separar Strings divididas por espaço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[selector, 'LastName'] = df.FirstName[selector].apply(lambda x: x.split(' ')[1])    #[1] = ocorrência após ' '\n",
    "df.loc[selector, 'FirstName'] = df.FirstName[selector].apply(lambda x: x.split(' ')[0])   #[0] = primeira ocorrencia antes ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"SubsChar\"></a>\n",
    "<b>Regular Expressions - Substituir Caracteres incoerentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "df.Height = df.Height.apply(lambda x: re.sub('[^0-9]', '', str(x)))       # Substitute non-numbers with ‘’\n",
    "df.Height = pd.to_numeric(df.Height, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "value = value.replace('$', '')\n",
    "df['price_no_dollar'] = df['price'].apply(lambda x: x.replace('$', '')                   # Replace '$' by ''\n",
    "df['price_no_dollar'] = df['price'].apply(lambda x: re.findall('\\d+\\.\\d+', x)[0])        # Copy only the pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing Values\n",
    "df.ColA.replace('?', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"CleanDup\"></a>\n",
    "<b>Apagando Duplicatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apagar duplicatas onde a chave única é um subset da feature\n",
    "df.drop_duplicates()         # all features\n",
    "df.drop_duplicates(subset=['FirstName', 'LastName', 'Gender', 'Age'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"TratNAN\"></a>\n",
    "<h2>Tratamento de NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"MissLib\"></a>\n",
    "<b>Biblioteca Missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import missingno as msno                                                  # Lib para Tratar Missing Values\n",
    "\n",
    "missingValueColumns = train.columns[train.isnull().any()].tolist()\n",
    "df_null = train[missingValueColumns] \n",
    "msno.heatmap(df_null,figsize=(20,8),cmap=colormap)                        # Heatmap de NaN\n",
    "\n",
    "sorted_data = msno.nullity_sort(df_null, sort='descending')               # or sort='ascending'\n",
    "msno.matrix(sorted_data,figsize=(20,8),fontsize=14)                       # Matrix de NaN    <---- COOL -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"FindNAN\"></a>\n",
    "<b>Achando NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.isnull()                                      # Exibe colunas com NaN\n",
    "df.isnull().sum()                                # Conta quantos NaNs por Coluna\n",
    "df[pd.isnull(df).any(axis=1)]                    # Exibe quais linhas possuem NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"FillNAN\"></a>\n",
    "<b>Preenchendo NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.fillna(0)                                          # Fill all NaN's with a scalar\n",
    "df[['ColA', 'ColB']] = df[['ColA', 'ColB']].fillna(0)\n",
    "\n",
    "df.my_feature.fillna( df.my_feature.mean() )\n",
    "df.my_feature.fillna( df.mean(axis=0) )               #  Fill NaN's with a mean\n",
    "\n",
    "df.fill( method='ffill', limit=1)                     # method='bfill' / limit = ## Fill Forward / Backward with a previous value\n",
    "\n",
    "df.interpolate(method='polynomial', order = 2)        # Interpolate w/ a polynomial - check nan position\n",
    "\n",
    "df.dropna(axis = 0)    /   df.dropna(axis = 1)        # Drop all Rows / Columns with holes\n",
    "df.dropna(how='any')   /  df.dropna(how='all')        # Drop if ANY value in the row has a nan\n",
    "df.dropna(axis = 1, how='any')\n",
    "\n",
    "df = df.dropna(axis=0, thresh=4)                      # Drop any row with NaNs that has at least 4 non-NaNs within it\n",
    "\n",
    "df = df.drop(labels=['ColA', 'ColB'], axis=1)         # Drop specific features with NaN.   Axis=1 for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXAMPLES\n",
    "bad_rows = df[df.Date.isnull()].index\n",
    "df.drop( bad_rows, inplace=True)\n",
    "\n",
    "df.Gender.fillna('M', inplace=True)\n",
    "df.Occupation.fillna('Unspecified', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Unique\"></a>\n",
    "<b>Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.Age.unique()                                        # to view a list\n",
    "df.Age.value_counts(dropna=False)                      # to know how many times each of those unique values are present\n",
    "df.Age.value_counts(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"AdjIndex\"></a>\n",
    "<h2>Transforming the Index</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.reset_index()                                                    # cria uma coluna com os índices originais\n",
    "df.reset_index(drop=True, inplace=True)                             # não cria coluna com índices originais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "series.index.name = 'Nome'                                          # Renomeia a coluna de índices\n",
    "stocks = stocks.set_index = ['Symbol', 'Date']                      # Colunas como Index. MultiIndex / Hierarchical\n",
    "\n",
    "stocks.loc[(slice(None), slice('2016-10-03', '2016-10-04')),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.reindex(year).ffill()                                     # Reindex df1 using the list year with forward-fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Swapping Index Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swapped = df.swaplevel(0, 1)     # Troca os indices 0 e 1\n",
    "sorted = swapped.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Chains\"></a>\n",
    "You can <b>chain methods</b> to write your operations more compactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0, thresh=2).drop(labels=['ColA', axis=1]).drop_duplicates(subset=['ColB', 'ColC']).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Globbing\"></a>\n",
    "<h3>Globbing</h3>\n",
    "Pattern matching for file names\n",
    "Wildcards: * ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "csv_files = glob.glob('*.csv')\n",
    "list_data = []\n",
    "for filename in csv_files:\n",
    "    data = pd.read_csv(filename)\n",
    "    list_data.append(data)\n",
    "pd.concat(list_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"RE\"></a>\n",
    "<h3>Regular Expressions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criar um padrão\n",
    "pattern = re.compile('\\$\\d*\\.\\d{2}')              # $12345.67\n",
    "CPF = re.compile('\\d{3}.\\d{3}.\\d{3}-\\d{2}')\n",
    "result = CPF.match('754.259.657-65')              # Verifica se está igual o padrão\n",
    "print(bool(CPF))\n",
    "\n",
    "# Anchor the pattern to match exactly what you want by placing a ^ in the beginning and $ in the end.\n",
    "\n",
    "pattern='\\$\\d*\\.\\d{2}'                            # A dollar sign, an arbitrary number of digits, a decimal point, 2 digits.\n",
    "pattern='[A-Z]\\w*'                                # A capital letter, followed by an arbitrary number of alphanumeric characters.\n",
    "pattern3 = bool(re.match(pattern='[A-Z]\\w*', string='Australia'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting numerical values from strings\n",
    "# \\d is the pattern required to find digits followed with + so that the previous element is matched one or more times.\n",
    "matches = re.findall('\\d+', 'the recipe calls for 10 strawberries and 1 banana') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tips['total_dollar_re'] = tips['total_dollar'].apply(lambda x: re.findall('\\d+\\.\\d+', x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re.match(padrão, string)                   # procura pelo padrão no inicio da string\n",
    "re.search(padrão, string)                  # procura pelo padrão em qualquer lugar da string\n",
    "re.findall(padrão, string)                 # retorna uma lista de substrings encontradas\n",
    "re.sub(padrão, novo, string, max=0)        # substitui os padrões por novo na string, se max=0 substitui todas as ocorrências\n",
    "\n",
    "Metacharacters:\n",
    "pattern = r\"gr.y\"                          # . é qualquer caracter                         resp: gray, grey, grwy,grxy...\n",
    "pattern = r\"^grey\"                         # coincide o padrão com o início (^) da string\n",
    "pattern = r\"grey$\"                         # coincide o padrão com o fim ($) da string\n",
    "pattern = r\"[aeiou]\"                       # padrão com lista de vogais.\n",
    "pattern = r\"[A-Z][A-Z][0-9]\"               # 3 digitos sendo o primeiro e o segundo com letras maiúsculas e o terceiro com digito\n",
    "pattern = r\"[^A-Z]\"                        # O ^ inverte o padrão, ou seja, nenhuma letra maiúscula\n",
    "pattern = r\"egg(spam)*\"                    #ZERO ou mais repetições do que está no ()      resp: egg ou eggspam ou eggspamspam...\n",
    "pattern = r\"(g)+\"                          #UMA ou mais repetições do que está no ()       resp: g ou ggggggggggg...\n",
    "pattern = r\"ice(-)?cream\"                  #ZERO ou UMA repetição                          resp: ice-cream ou ice--cream\n",
    "pattern = r\"9{1,3}$\"                       #Entre 1 ou 3 repetições do 9\n",
    "# Use A-Za-z to match the set of lower and upper case letters, \\. to match periods, and \\s to match whitespace between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Apply\"></a>\n",
    "<h2>Apply</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.apply(np.mean, axis=0)                         # Aplica a média na coluna\n",
    "df['ColX'] = df.ColA.apply(function)              # passa a função function pelo Apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Lambda\"></a>\n",
    "<h2>Lambda Function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "power = lambda x, y: x ** y                                            # Lambda function\n",
    "\n",
    "add_bangs = (lambda a: a + '!!!'), and the function call is: add_bangs('hello')\n",
    "\n",
    "fellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']\n",
    "result = filter(lambda member: len(member) > 6, fellowship)            # filter function, select as criteria\n",
    "\n",
    "stark = ['robb', 'sansa', 'arya', 'eddard', 'jon']\n",
    "result = reduce(lambda item1, item2: item1 + item2, stark)             # reduce() function --> concatenates\n",
    "\n",
    "df['price_no_dollar'] = df['price'].apply(lambda x: x.replace('$', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Assert\"></a>\n",
    "<h2>Assert Statements</h2>\n",
    "Retorna um erro caso ocorra a condição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert df.ColA.notnull().all()                          # Erro se houver Null\n",
    "assert pd.notnull(df).all().all                         # Erro se houver Null\n",
    "assert df['Life'].value_counts()[0] == 1                # Erro se houver Null\n",
    "assert (df >= 0).all().all()                            # Assert that all values are >= 0\n",
    "assert gapminder.year.dtype == np.int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exceptions / Error handling:\n",
    "\n",
    "try:                    # Executa o bloco abaixo\n",
    "   comando ou bloco\n",
    "except ImportError:     # Caso alguma exceção ocorra: ImportError, IndexError, NameError, SyntaxError, TypeError\n",
    "   comando ou bloco\n",
    "finally:                # Executa o bloco abaixo de qualquer maneira, incluindo se ocorrer exceção\n",
    "   comando ou bloco\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"TimeSeries\"></a>\n",
    "<h2>Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts1 = ts0.loc['2010-10-11 21:00:00']           # Extract the hour from 9pm to 10pm on '2010-10-11': ts1\n",
    "ts2 = ts0.loc['July 4th, 2010']                # Extract '2010-07-04' from ts0: ts2\n",
    "ts3 = ts0.loc['12/15/2010':'12/31/2010']       # Extract data from '2010-12-15' to '2010-12-31': ts3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resampling pandas time series                                                   # 'min', 'T' = minute\n",
    "# DOWNSAMPLING                                                                    # 'H' = hour\n",
    "daily_mean = sales.resample('D').mean()     # D = daily                           # 'D' = day\n",
    "sales.resample('D').sum().max()             # Max sales                           # 'B' = business day\n",
    "sales.resample('W').count()                 # Counts of sales Weekly              # 'W' = week                  2W  = 2 Weeks\n",
    "                                                                                  # 'M' = month\n",
    "                                                                                  # 'Q' = quarter\n",
    "                                                                                  # 'A' = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = df['Temperature'].resample('6h').mean()            # Downsample to 6 hour data and aggregate by mean\n",
    "df2 = df['Temperature'].resample('D').count()            # Downsample to daily data and count the number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rolling mean and frequency\n",
    "hourly_data.rolling(window=24).mean()\n",
    "# Use a rolling 7-day window with method chaining to smooth the daily high temperatures in August\n",
    "daily_highs_smoothed = august.resample(\"D\").max().rolling(window=7).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales['Product'].str.contains('ware').sum()\n",
    "sales['Date'].dt.hour                                                  # Retorna uma lista de 0-23 com ocorrencias nas horas\n",
    "df.columns = df.columns.str.strip(' ')                                 # Strip extra whitespace from the column names\n",
    "dallas = df['Destination Airport'].str.contains('DAL')                 # Extract data for which the destination airport is Dallas\n",
    "overcast = df_clean.loc[df_clean['condition'].str.contains('OVC')]     # Select days that are overcast: overcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"Stats\"></a>\n",
    "<h2>Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()                                    # Vários dados\n",
    "df.cov()                                         # Covariancia\n",
    "df.mean()\n",
    "df.median()\n",
    "df.std()\n",
    "df.quantile(0.5)                                 # Quantile 50%\n",
    "df.min()\n",
    "df.max()\n",
    "\n",
    "np.mean(np_array2[: ,1])                         # média dos valores da 2ª coluna\n",
    "np.median(np_array2[: ,1])                       # mediana dos valores da 2ª coluna\n",
    "np.std( np_array2[: ,1]))                        # desvio padrão\n",
    "np.percentiles(array, [25, 50, 75])              # value for 25, 50 and 75 percentiles\n",
    "np.var(array)                                    # Variância\n",
    "np.cov(array1, array2)                           # Covariância\n",
    "np.corrcoef(np_array2[: ,0], np_array2[: ,1])    # coeficiente de correlação --- Pearson correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTRIBUITIONS\n",
    "# The Poisson distribution is a limit of the Binomial distribution\n",
    "# when the probability of success is small and the number of Bernoulli trials is large.\n",
    "\n",
    "np.random.binomial(num_trial, prob_success, size=num_samples)       # Binomial distribution\n",
    "np.random.poisson (mean, size=num_samples)                          # Poisson distribution\n",
    "np.random.normal(mean, std, size=)                                  # Normal\n",
    "np.random.exponential(mean, size=)                                  # Exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEADS or TAILS\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "random_numbers = np.random.random(size=4)\n",
    "random_numbers\n",
    "heads = random_numbers < 0.5\n",
    "heads\n",
    "np.sum(heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Empirical cumulative distribution functions (ECDF)\n",
    "import numpy as np\n",
    "x = np.sort(df_swing['dem_share'])\n",
    "y = np.arange(1, len(x)+1) / len(x)\n",
    "g = plt.plot(x, y, marker='.', linestyle='none')\n",
    "g = plt.ylabel('ECDF')\n",
    "g = plt.xlabel('% of vote for Obama')\n",
    "plt.margins(0.02)   # Keeps data off plot edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECDF Function\n",
    "def ecdf(data):\n",
    "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n",
    "    n = len(data)                # Number of data points    \n",
    "    x = np.sort(data)            # x-data for the ECDF    \n",
    "    y = np.arange(1, n+1) / n    # y-data for the ECDF\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0, max(n_defaults) + 1.5) - 0.5         # Compute bin edges centered on the integers\n",
    "plt.hist(n_defaults, normed=True, bins=bins)             # Generate histogram\n",
    "\n",
    "plt.hist(samples_std1, normed=True, histtype='step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slope = \"angle of the curve\"\n",
    "# intercept = point where line cross y\n",
    "# residual = how far the point in from the line (negative if below line)\n",
    "\n",
    "# 1 is the degree of the polinom you wish to fit | 1 for linear functions\n",
    "slope, intercept = np.polyfit(x=total_votes, y=dem_share, 1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bs_pairs_linreg(x, y, size=1):\n",
    "    \"\"\"Perform pairs bootstrap for linear regression.\"\"\"\n",
    "\n",
    "    # Set up array of indices to sample from\n",
    "    inds = np.arange(len(x))\n",
    "\n",
    "    # Initialize samples\n",
    "    bs_slope_reps = np.empty(size)\n",
    "    bs_intercept_reps = np.empty(size)\n",
    "\n",
    "    # Take samples\n",
    "    for i in range(size):\n",
    "        bs_inds = np.random.choice(inds, len(inds))\n",
    "        bs_x, bs_y = x[bs_inds], y[bs_inds]\n",
    "        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)\n",
    "\n",
    "    return bs_slope_reps, bs_intercept_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anscombe's quartet !!!!! same mean, same intercept, same slope, same regression line, different shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOOTSTRAPPING\n",
    "# The use of resampled data to perform statistical inference\n",
    "np.random.choice([1,2,3,4,5], size=5)     # escolhe 5 números do array, pode repetir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard error of the mean, or SEM\n",
    "sem = np.std(array) / np.sqrt(len(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Generate array of x-values for bootstrap lines: x\n",
    "x = np.array([0, 100])\n",
    "\n",
    "# Plot the bootstrap lines\n",
    "for i in range(0,100):\n",
    "    _ = plt.plot(x, bs_slope_reps[i]*x + bs_intercept_reps[i], linewidth=0.5, alpha=0.2, c='red')\n",
    "\n",
    "# Plot the data\n",
    "_ = plt.plot(illiteracy, fertility, marker='.', linestyle='none')\n",
    "\n",
    "# Label axes, set the margins, and show the plot\n",
    "_ = plt.xlabel('illiteracy')\n",
    "_ = plt.ylabel('fertility')\n",
    "plt.margins(0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Outliers\"></a>\n",
    "<b>OUTLIERS - Remove top 5 and bottom 5 samples for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = {}\n",
    "for col in df.columns:\n",
    "    # Bottom 5\n",
    "    sort = df.sort_values(by=col, ascending=True)\n",
    "    if len(sort) > 5: sort=sort[:5]\n",
    "    for index in sort.index: drop[index] = True # Just store the index once\n",
    "\n",
    "    # Top 5\n",
    "    sort = df.sort_values(by=col, ascending=False)\n",
    "    if len(sort) > 5: sort=sort[:5]\n",
    "    for index in sort.index: drop[index] = True # Just store the index once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"OBS\"></a>\n",
    "<h1>OBS:</h1>\n",
    "\n",
    "<b>Pandas</b><br>\n",
    "<a href=\"https://pandas.pydata.org/pandas-docs/stable/10min.html\">10 minutes to Pandas</a><br>\n",
    "<a href=\"http://pandas.pydata.org/pandas-docs/stable/tutorials.html\">General Tutorials</a><br>\n",
    "<a href=\"http://pandas.pydata.org/pandas-docs/stable/cookbook.html\">Cookbooks</a><br>\n",
    "<a href=\"http://chrisalbon.com/#Python\">39 Data Wrangling Techniques</a><br>\n",
    "<a href=\"http://pandas.pydata.org/pandas-docs/stable/missing_data.html\">Working with Missing Data, Including Interpolation</a><br>\n",
    "<a href=\"http://scikit-learn.org/stable/auto_examples/plot_missing_values.html#sphx-glr-auto-examples-plot-missing-values-py\">Imputation of Missing Values</a><p>\n",
    "\n",
    "<b>How to Select Good Features?</b><br>\n",
    "<a href=\"http://machinelearningmastery.com/an-introduction-to-feature-selection/\">An Introduction to Feature Selection</a><br>\n",
    "<a href=\"https://courses.washington.edu/css490/2012.Winter/lecture_slides/05a_feature_creation_selection.pdf\">Jeff Howbert Intro to Machine Learning / Feature Creation</a><br>\n",
    "<a href=\"http://cs229.stanford.edu/materials/ML-advice.pdf\">Andrew Ng - Advice for Applying Machine Learning</a><p>\n",
    "\n",
    "<b>Where to Find Good / Free Datasets?</b><br>\n",
    "<a href=\"http://data.gov/\">Data.gov</a><br>\n",
    "<a href=\"https://archive.ics.uci.edu/ml/datasets.html\">UCI ML Datasets (Mirror)</a><br>\n",
    "<a href=\"https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public?q=dataset\">Quora Answers</a><p>\n",
    "\n",
    "<b>SciKit-Stuff</b><br>\n",
    "<a href=\"https://scikits.appspot.com/scikits\">List of SciKits</a> (In addition to SciKit-Learn)<br>\n",
    "<a href=\"https://stackoverflow.com/questions/17818783/populate-a-pandas-sparsedataframe-from-a-scipy-sparse-matrix#17819427\">Sparse Matrix Conversions For Binary Datasets</a><p>\n",
    "\n",
    "<b>Feature Extraction</b><br>\n",
    "<a href=\"http://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation\">SciKit-Learn Feature Extraction</a><br>\n",
    "<a href=\"http://www.ifs.tuwien.ac.at/~schindler/lectures/MIR_Feature_Extraction.html\">Audio Feature Extraction (Many Methods)</a><p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Pandas, Matplotlib, Seaborn, ...\n",
    "https://bokeh.pydata.org/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXEMPLOS\n",
    "\n",
    "countries = gapminder['country']                                # Create the series of countries: countries\n",
    "countries = countries.drop_duplicates()                         # Drop all the duplicates from countries\n",
    "pattern = '^[A-Za-z\\.\\s]*$'                                     # Write the regular expression: pattern\n",
    "mask = countries.str.contains(pattern)                          # Create the Boolean vector: mask\n",
    "mask_inverse = ~mask                                            # Invert the mask: mask_inverse\n",
    "invalid_countries = countries.loc[mask_inverse]                 # Subset countries using mask_inverse: invalid_countries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
