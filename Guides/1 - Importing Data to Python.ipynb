{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "wd = os.getcwd()      # stores the name of the current directory \n",
    "os.listdir(wd)        # outputs the contents of the directory\n",
    "chdir=('new\\\\path')\n",
    "\n",
    "! ls       # starting a line with ! gives you complete system shell access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PYTHON\n",
    "filename = 'file.txt'\n",
    "file = open(filename, mode = 'r')            # r = read-only; w =write mode; b = binary file; a = append mode\n",
    "\n",
    "text = file.read()                           # Ler todo o conteúdo\n",
    "ler32bytes = arquivo.read(32)                # ler 32 bytes\n",
    "lerCadaLinha = arquivo.readlines()           # Ler cada linha\n",
    "\n",
    "print (text)\n",
    "print (file.closed)                          # Check whether file is closed   True / False\n",
    "file.close()                                 # Close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Context manager\n",
    "with open(filename, mode='r') as file:\n",
    "    print (file.readline())\n",
    "    print (file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing data in NUMPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NUMPY\n",
    "import numpy as np\n",
    "file = 'arq.txt'\n",
    "data = np.loadtxt(file, delimiter=',', skiprows=1, usecols=[0, 2], dtype=str)\n",
    "data = np.genfromtxt('titanic.csv', delimiter=',', names=True, dtype=None)\n",
    "data = np.recfromcsv(file, delimiter='\\t', names=True)                                  # default dtype is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Matrix has rows and columns\n",
    "A Data Frame has observations and variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importing data in PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\DIR\\Ses_seguros.csv',\n",
    "                 sep=';',\n",
    "                 thousands='.',\n",
    "                 decimal=',',\n",
    "                 index_col='Damesano',                            # ou index_col=False\n",
    "                 names=['Quando', 'cabeçalhos', 'sem', 'nome'],\n",
    "                 na_values='-1',                                  # ou na_values=['ColA':['-1']]\n",
    "                 parse_dates=[[0, 1, 2]]                          # Quais colunas compõem uma data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PANDAS\n",
    "import pandas as pd\n",
    "file = 'arq.cvs'\n",
    "data = pd.read_csv(file, header=None, nrows=5, sep='\\t', comment='#', na_values='Nothing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = ['file1.csv', 'file2.csv']\n",
    "dataframe = [pd.read_csv(f) for f in filenames]\n",
    "\n",
    "import g\n",
    "filenames = ['file1.csv', 'file2.csv']\n",
    "dataframe = [pd.read_csv(f) for f in filenames]\n",
    "\n",
    "dataframes = []\n",
    "for filename in filenames:\n",
    "    dataframes.append(pd.read_csv(filenames))\n",
    "\n",
    "ColA.reindex(ColB.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Pickled files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickled files\n",
    "import pickle\n",
    "with open('pickled_fruit.pkl', 'rb') as file:          # 'rb' read-only and binary\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Excel files\n",
    "data = pd.ExcelFile(file)\n",
    "print (data.sheet_names)                               # Exibe as abas da planilha\n",
    "df1 = data.parse('aba1')\n",
    "df2 = data.parse(0, parse_cols=[0], skiprows=[0], names=['Country'])\n",
    "\n",
    "xl = pd.read_excel('file.xls', sheetname=None)         # to import all sheets -> sheetname=None\n",
    "print (xl.keys())                                      # Exibe as abas da planilha\n",
    "print (xl['aba1'].head())                              # Exibe head da aba1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df   = pd.read_excel('my_dataset.xlsx', 'Sheet1', na_values=['NA', '?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SAS / Stata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SAS / Stata\n",
    "import pandas as pd\n",
    "from sas7bdat import SAS7BDAT\n",
    "with SAS7BDAT('urban.sas7bdat') as file:\n",
    "df_sas = file.to_data_frame()\n",
    "\n",
    "data = pd.read_stata('urban.dta')          # Stata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>HDF5 files -- Hierarchical Data Format version 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HDF5 files -- Hierarchical Data Format version 5\n",
    "import h5py\n",
    "filename = 'arq.hdf5'\n",
    "data = h5py.File(filename, 'r')            # 'r' to read, 'w' to write\n",
    "\n",
    "for key in data.keys():\n",
    "    print (key)       # meta, quality, strain\n",
    "\n",
    "for key in data['meta'].keys():\n",
    "    print (key)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>MATLAB files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MATLAB files  -- Arquivos como Dicionarios\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('filename.mat')     # read  .mat files\n",
    "mat = scipy.io.savemat('filename.mat')     # write .mat files\n",
    "print (mat.keys())                         # Exibe as chaves do dict\n",
    "print (type(mat['Nome']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SQL query via Pandas\n",
    "df = pd.read_sql_query('SELECT * FROM Orders', engine)\n",
    "# df   = pd.read_sql_table('my_table', engine, columns=['ColA', 'ColB'])\n",
    "\n",
    "# INNER JOIN via SQL\n",
    "df = pd.read_sql_query('SELECT OrderID, Name FROM Orders INNER JOIN Customers on Orders.CustID = Customers.CustID', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SQL engine -- DIRECT -- Importing to df\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "engine = create_engine('sqlite:///Database.sqlite')\n",
    "table_names = engine.table_names()\n",
    "print (table_names)\n",
    "\n",
    "connection = engine.connect()                                   # query = 'SELECT * FROM people'\n",
    "results = connection.execute(\"SELECT * FROM Table\")             # results = connection.execute(query1).fetchall()\n",
    "df = pd.DataFrame(rs.fetchall())\n",
    "df.columns = rs.keys()\n",
    "con.close()\n",
    "\n",
    "query2 = select([census])           # Also select\n",
    "\n",
    "\n",
    "first_row = results[0]\n",
    "print(first_row)\n",
    "print(first_row.keys())\n",
    "print(first_row.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SQL engine -- Context manager\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "engine = create_engine('sqlite:///Database.sqlite')\n",
    "\n",
    "with engine.connect() as con:                                             # No need to Close()\n",
    "    rs = con.execute(\"SELECT OrderID, OrderDate, ShipName FROM Orders\")\n",
    "    df = pd.DataFrame(rs.fetchmany(size=5))                               # fetch 5 rows\n",
    "    df.columns = rs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# METADATA\n",
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "engine = create_engine('sqlite:///census.sqlite')\n",
    "\n",
    "print(engine.table_names())                                                 # Print the table names\n",
    "metadata = MetaData()                                                       # Select Metadata\n",
    "census = Table('census', metadata, autoload=True, autoload_with=engine)     # Reflect census table via engine:\n",
    "print(repr(census)                                                          # show table details\n",
    "print(census.columns.keys())                                                # Print the column names\n",
    "print(repr(metadata.tables['census']))                                      # Print full table metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stmt = select([census])                                                    # Select * from census\n",
    "stmt = stmt.where(census.column.state == 'CA')                             # where state == 'CA'\n",
    "results = connection.execute(stmt).fetchall()\n",
    "for result in results:\n",
    "    print (result.state, result.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# usr: student / pwd: datacamp\n",
    "engine = create_engine('postgresql+psycopg2://student:datacamp@postgresql.csrrinzqubik.us-east-1.rds.amazonaws.com:5432/census')\n",
    "engine = create_engine('mysql+pymysql://student:datacamp@courses.csrrinzqubik.us-east-1.rds.amazonaws.com:3306/census')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formato de Dados para leitura<br>\n",
    "<a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\">Read_csv documentation</a><br>\n",
    "<a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html\">Read_excel documentation</a><br>\n",
    "<a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_html.html\">Read_html documentation</a><br>\n",
    "<a href=\"http://pandas.pydata.org/pandas-docs/version/0.20/generated/pandas.read_sql.html\">Read_sql documentation</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Gravando o conteúdo do DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gravando o conteúdo do DataFrame\n",
    "df.to_sql('table', engine)\n",
    "df.to_excel('dataset.xlsx')\n",
    "df.to_json('dataset.json')\n",
    "df.to_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importing data from WEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df= pd.read_html('http://page.com/with/table.html')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WEB\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "url = 'http://........./arq.csv'\n",
    "urlretrieve (url, 'arq.csv')              # Save file locally\n",
    "\n",
    "df = pd.read_csv(url, sep=',')\n",
    "x1 = pd.read_excel(url, sheetname=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>GET requests using URLLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "\n",
    "url = 'https://www.wikipedia.org/'\n",
    "request = Request(url)\n",
    "response = urlopen(request)          # accepts URLs instead of file names\n",
    "html = response.read()\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>GET requests using REQUESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.wikipedia.org/'\n",
    "r = requests.get(url)\n",
    "text = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapping with <b>BeautifulSoup</b> -- Parse and extract data from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautfulSoup\n",
    "import requests\n",
    "\n",
    "url = 'http://....'\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautfulSoup(html_doc)\n",
    "\n",
    "print (soup.prettify())\n",
    "print (soup.title)\n",
    "print (soup.get_text())\n",
    "\n",
    "for link in soup.find_all('a'):       # Find all tags with 'a'\n",
    "    print (link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('file.json', 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "for key, value in json_data.items():\n",
    "    print (key + \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df  = pd.read_json('my_dataset.json', orient='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>APIs and interacting with the world wide web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://www.omdbapi.com/?t=hackers'\n",
    "r = requests.get(url)\n",
    "json_data = r.json()\n",
    "for key, value in json_data.items():\n",
    "    print (key + \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://www.omdbapi.com/?apikey=ff21610b&t=social+network'\n",
    "r = requests.get(url)        # Package the request, send the request and catch the response\n",
    "print(r.text)                # Print the text of the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
    "\n",
    "r = requests.get(url)\n",
    "json_data = r.json()         # Decode the JSON data into a dictionary\n",
    "\n",
    "pizza_extract = json_data['query']['pages']['24768']['extract']     # Print the Wikipedia page extract\n",
    "print (pizza_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy, json\n",
    "\n",
    "access_token = '...'\n",
    "access_token_secret = '...'\n",
    "consumer_key = '...'\n",
    "consumer_secret = '...'\n",
    "\n",
    "# DataCamp Example\n",
    "# Store OAuth authentication credentials in relevant variables\n",
    "# access_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\n",
    "# access_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n",
    "# consumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\n",
    "# consumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define stream listener class\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    def __init__(self, api=None):\n",
    "        super(MyStreamListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "        self.file = open(\"tweets.txt\", \"w\")            # creates a file\n",
    "\n",
    "    def on_status(self, status):\n",
    "        tweet = status.json\n",
    "        self.file.write(json.dumps(tweet) + '\\n')      # write to file\n",
    "        tweet_list.append(status)\n",
    "        self.num_tweets += 1\n",
    "        if self.num_tweets < 100:                      # Onde 100 tweets\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        self.file.close()                              # Close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Streaming object and authenticate\n",
    "l = MyStreamListener()\n",
    "stream = tweepy.Stream(auth, l)\n",
    "\n",
    "# This line filters Twitter Streams to capture data by keywords:\n",
    "stream.filter(track=['apples', 'oranges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
