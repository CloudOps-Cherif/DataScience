{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Supervised Learning</b> - Classification & Regression<br>\n",
    "\n",
    "Numeric features should be scaled (Z-scored)<br>\n",
    "Categorical features should be encoded (One-hot)<p>\n",
    "\n",
    "XGBoost uses CART - Cassification and Regression Trees<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> When should I use XGBoost</b><br>\n",
    "<pre>\n",
    "Should:                                                       Shouldn't:\n",
    "-samples > 1000 training ans < 100 feats                     - Image recon, NLP, computer vision\n",
    "-num feats < num training samples                            - Few training samples\n",
    "-categorical and numeric feats \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> LOSS FUNCTIONS</b><br>\n",
    "Quantifies how far off a prediction is from the actual result<p>\n",
    "\n",
    "reg:linear - use for Refression<br>\n",
    "reg:logistic - use for Classification, when you want just <decision>, not probability<br>\n",
    "binary:logistic - use for Classification, when you want <probability> rather than just decision<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> BASE LEARNERS</b><br>\n",
    "Individual models to ensemble -  (i.e. tree and linear)<p>\n",
    "    \n",
    "Want base learners that when combined create final prediction that is non-linear<br>\n",
    "Each base learner should be good at distinguishing or predictiong different parts of the dataset<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>DENSE MATRIX</b><br>\n",
    "DMatrix são criadas durante o processo do XGBoost, mas para usar CV é necessário converter antes.<br>\n",
    "DMatrix são estruturas otimizadas para XGB.<br>\n",
    "```MD_train = xgb.DMatrix(data=X_train, label=y_train)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLASSIFICATION\n",
    "\n",
    "'''METRICS:\n",
    "Binary classification model      => AUC\n",
    "Multi-class classification model => Accuracy '''\n",
    "\n",
    "\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "xg_cl = xgb.XGBClassifier(objective='reg:logistic',    n_estimators=10, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REGRESSION\n",
    "\n",
    "'''METRICS:\n",
    "# Error = Actual - Predicted\n",
    "# Root mean squared error (RMSE) =>  sqrt(mean(sum((Error)**2))) - punishes larger diff between actual and pred\n",
    "# Mean absolute error (MAE)      =>  mean(sum(abs(Error))) '''\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Selecting Base Learners\n",
    "# Note: booster='gbtree' is the default\n",
    "\n",
    "MD_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "MD_test  = xgb.DMatrix(data=X_test,  label=y_test)\n",
    "\n",
    "params = {'booster':'gblinear', 'objective':'reg:linear'}              # gblinear for linear; booster=\"gbtree\" for trees\n",
    "\n",
    "xg_reg = xgb.train(params = params, dtrain = DM_train, num_boost_round = 10)\n",
    "preds = xg_reg.predict(DM_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the rmse\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Plotting XGBoost trees</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "\n",
    "# Plot the FIRST tree\n",
    "xgb.plot_tree(xg_reg, num_trees=0)\n",
    "plt.show()\n",
    "\n",
    "# Plot the FIFTH tree\n",
    "xgb.plot_tree(xg_reg, num_trees=4)\n",
    "plt.show()\n",
    "\n",
    "# Plot the LAST TREE SIDEWAYS\n",
    "xgb.plot_tree(xg_reg, num_trees=9, rankdir=\"LR\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the FEATURE IMPORTANCES\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>FINE-TUNING</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>REGULARIZATION</b> - is a control on model complexity<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>COMMON TREE TUNABLE PARAMS:</b><br>\n",
    "<b>learning rate</b> - [0.001, 0.01, 0.1]<br>\n",
    "<b>gamma</b> - min loss reduction to create new tree split (Regularization)<br>\n",
    "<b>alpha</b> - L1 reg on leaf weights, larger values mean more Regularization - [1, 10, 100]<br>\n",
    "<b>lambda</b> - L2 reg on leaf weights, smoother Regularization<br>\n",
    "<b>max_depth</b> - max depth per tree - [2, 5, 10, 20]<br>\n",
    "<b>subsample</b> - % samples used per tree (low or high can overfit)<br>\n",
    "<b>colsample_bytree</b> - % feats used per tree (low is additional Regularization, high can overfit) = max_features in RandomForest - [0.1, 0.5, 0.8, 1]<p>\n",
    "\n",
    "<b>LINEAR TUNABLE PARAMS:</b><br>\n",
    "<b>alpha</b> - L1 reg on leaf weights (Regularization)<br>\n",
    "<b>lambda</b> - L2 reg on leaf weights (Regularization)<br>\n",
    "<b>lambda_bias</b> - LG reg term on bias<p>\n",
    "\n",
    "<b>estimators numbers</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CROSS-VALIDATION in XGBoost\n",
    "''' # DMatrix são criadas durante o processo do XGBoost, mas para usar CV é necessário converter antes.\n",
    "      DMatrix são estruturas otimizadas para XGB.'''\n",
    "\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "\n",
    "params = {'objective':'binary:logistic',\n",
    "          'max_depth':4,\n",
    "          'colsample_bytree':0.3,\n",
    "          'learning_rate':0.1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=DM_train,                   # data to train\n",
    "                    params=params,                     # params dict\n",
    "                    nfold=4,                           # num of folds\n",
    "                    num_boost_round=10,                # num of trees\n",
    "                    early_stopping_rounds=10,          # early_stopping\n",
    "                    metrics='error',                   # metric\n",
    "                    as_pandas=True,                    # output as DataFrame\n",
    "                    seed=42)                           # random seed\n",
    "\n",
    "\n",
    "#metrics = error, auc, rmse, mae\n",
    "\n",
    "print(cv_results)                                     # Print cv_results\n",
    "print((cv_results[\"test-mae-mean\"]).tail(1))          # Extract and print final boosting round metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REGULARIZATION - is a control on model complexity\n",
    "# gamma - minimum loss reduction allowed for a split to occur\n",
    "# alpha - l1 regularization on leaf weights, larger values mean more regularization\n",
    "# lambda - l2 regularization on leaf weight (smoother regularization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRIDSEARCH\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gbm_param_grid ={ 'learing_rate'    : [0.01, 0.1, 0.5 , 0.9],\n",
    "                  'n_estimators'    : [200],\n",
    "                  'subsample'       : [0.3, 0.5, 0.9],\n",
    "                  'colsample_bytree': [0.3, 0.7]}\n",
    "\n",
    "gbm = xgb.XGBRegressor()\n",
    "grid_mse = GridSearchCV(estimator = gbm,\n",
    "                        param_grid = gbm_param_grid,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        cv=4,\n",
    "                        verbose=1)\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "print ('Best params found: ', grid_mse.best_params_)\n",
    "print ('Lowest RMSE found: ', np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RANDOMSEARCH\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "gbm_param_grid ={ 'learing_rate': np.arange(0.05, 1.05, 0.5),\n",
    "                  'n_estimators': [200],\n",
    "                  'subsample'   : np.arange(0.05, 1.05, 0.5)}\n",
    "\n",
    "gbm = xgb.XGBRegressor()\n",
    "randomized_mse = RandomizedSearchCV(estimator = gbm,\n",
    "                                    param_distributions = gbm_param_grid,\n",
    "                                    n_iter=25,                                  # number of random combinations\n",
    "                                    scoring='neg_mean_squared_error',\n",
    "                                    cv=4,\n",
    "                                    verbose=1)\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "print ('Best params found: ', randomized_mse.best_params_)\n",
    "print ('Lowest RMSE found: ', np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search space size can be massive for Grid Search in certain cases, whereas for Random Search the number of hyperparameters has a significant effect on how long it takes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PREPROCESSING</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LABELENCODER\n",
    "\"\"\" LabelEncoder - Converts a categorical column of strings into integers \"\"\"\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df.LotFrontage = df.LotFrontage.fillna(0)                    # Fill missing values with 0\n",
    "categorical_mask = (df.dtypes == object)                     # Create a boolean mask for categorical columns\n",
    "categorical_columns = df.columns[categorical_mask].tolist()  # Get list of categorical column names\n",
    "print(df[categorical_columns].head())                        # Print the head of the categorical columns\n",
    "\n",
    "le = LabelEncoder()                                          # Create LabelEncoder object\n",
    "df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x)) # Apply LabelEncoder\n",
    "\n",
    "print(df[categorical_columns].head())                        # Print the head of the LabelEncoded categorical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONEHOTENCODER\n",
    "\"\"\" OneHotEncoder - Takes the column on integers and encodes them as dummy variables \"\"\"\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_mask = (df.dtypes == object)\n",
    "categorical_columns = df.columns[categorical_mask].tolist()\n",
    "\n",
    "ohe = OneHotEncoder(categorical_features=categorical_mask, sparse=False)   # Create OneHotEncoder\n",
    "df_encoded = ohe.fit_transform(df)                                         # output is no longer a dataframe\n",
    "\n",
    "print(df_encoded[:5, :])                                                   # Print first 5 rows of the resulting dataset\n",
    "\n",
    "print(df.shape)                                                            # Print the shape of the original DataFrame\n",
    "print(df_encoded.shape)                                                    # Print the shape of the transformed array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DictVectorizer\n",
    "\"\"\" DictVectorizer - Converts lists of feature mappings into vectors \"\"\"\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "df_dict = df.to_dict('records')               # Convert df into a dictionary\n",
    "\n",
    "dv = DictVectorizer(sparse=False)             # Create the DictVectorizer object\n",
    "df_encoded = dv.fit_transform(df_dict)        # Apply dv on df\n",
    "\n",
    "print(df_encoded[:5,:])                       # Print the resulting first five rows\n",
    "print(dv.vocabulary_)                         # Print the vocabulary (maps the names of the features to their indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PIPELINES</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X = X.to_dict(\"records\")\n",
    "\n",
    "# Setup the pipeline steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective=\"reg:linear\"))]\n",
    "\n",
    "# Create the pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Cross-validate the model\n",
    "cross_val_scores = cross_val_score(xgb_pipeline, X, y, cv=10, scoring='neg_mean_squared_error', )\n",
    "\n",
    "# Print the 10-fold RMSE\n",
    "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))\n",
    "\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn_pandas import CategoricalImputer\n",
    "\n",
    "\n",
    "nulls_per_column = X.isnull().sum()                                       # Check number of nulls in each feature column\n",
    "print(nulls_per_column)\n",
    "\n",
    "categorical_feature_mask = X.dtypes == object                             # Create a boolean mask for categorical columns\n",
    "categorical_columns = X.columns[categorical_feature_mask].tolist()        # Get list of categorical column names\n",
    "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()   # Get list of non-categorical column names\n",
    "\n",
    "\n",
    "# Apply numeric imputer\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "                                            [([numeric_feature], Imputer(strategy=\"median\")) \\\n",
    "                                             for numeric_feature in non_categorical_columns],\n",
    "                                            input_df=True,\n",
    "                                            df_out=True\n",
    "                                           )\n",
    "\n",
    "# Apply categorical imputer\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "                                                [(category_feature, CategoricalImputer(category_feature))\\\n",
    "                                                 for category_feature in categorical_columns],\n",
    "                                                input_df=True,\n",
    "                                                df_out=True\n",
    "                                               )\n",
    "\n",
    "# Import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "                                          (\"num_mapper\", numeric_imputation_mapper),\n",
    "                                          (\"cat_mapper\", categorical_imputation_mapper)\n",
    "                                         ])\n",
    "\n",
    "# Create full pipeline\n",
    "pipeline = Pipeline([\n",
    "                     (\"featureunion\", numeric_categorical_union),\n",
    "                     (\"dictifier\", Dictifier()),\n",
    "                     (\"vectorizer\", DictVectorizer(sort=False)),\n",
    "                     (\"clf\", xgb.XGBClassifier())\n",
    "                    ])\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_scores = cross_val_score(pipeline, X, y, scoring =\"roc_auc\", cv=3)\n",
    "\n",
    "# Print avg. AUC\n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>STUDY CASE</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "   \n",
    "names = [\"crime\",\"zone\",\"industry\",\"charles\",\"no\", \"rooms\",\"age\", \"distance\",\"radial\",\"tax\", \"pupil\",\"aam\",\"lower\",\"med_price\"]\n",
    "data = pd.read_csv(\"boston_housing.csv\",names=names)\n",
    "\n",
    "X, y = data.iloc[:,:-1],data.iloc[:,-1]\n",
    "\n",
    "xgb_pipeline = Pipeline[(\"st_scaler\", StandardScaler()),\n",
    "                        (\"xgb_model\", xgb.XGBRegressor())]\n",
    "   \n",
    "gbm_param_grid = {'xgb_model__learning_rate'   : np.arange( 0.05, 1, 0.05),\n",
    "                  'xgb_model__subsample'       : np.arange( .05, 1, .05),\n",
    "                  'xgb_model__max_depth'       : np.arange( 3, 20, 1),\n",
    "                  'xgb_model__colsample_bytree': np.arange( 0.1, 1.05, .05),\n",
    "                  'xgb_model__n_estimators'    : np.arange( 50, 200, 50)}\n",
    "   \n",
    "randomized_neg_mse = RandomizedSearchCV(estimator = xgb_pipeline,\n",
    "                                        param_distributions = gbm_param_grid,\n",
    "                                        n_iter=10,\n",
    "                                        scoring='neg_mean_squared_error',\n",
    "                                        cv=4,\n",
    "                                        verbose=1)\n",
    "randomized_neg_mse.fit(X, y)\n",
    "\n",
    "print(\"Best rmse: \", np.sqrt(np.abs(randomized_neg_mse.best_score_)))\n",
    "print(\"Best model: \", randomized_neg_mse.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> NEXT STEPS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using XGBoost for <b>ranking/recommendation</b> problems (Netflix/Amazon problem) ==> Modify Loss Function (?)<br>\n",
    "- <b>Bayesian Optimization</b> ==> Using more sophisticated hyperparameter tuning strategies for tuning XGBoost models<br>\n",
    "- Using XGBoost as part of an <b>ensemble</b> of other models for regression/classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
